{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Optimization and Parallelization\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/computer-vision/blob/main/chapter_18_optimization_and_parallelization.ipynb)\n",
    "\n",
    "**Performance optimization** is critical for production rendering. This chapter covers algorithmic optimizations, multi-threading, SIMD vectorization, and GPU acceleration strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Profiling and Measurement\n",
    "\n",
    "**Always profile before optimizing!**\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "- **Time**: Wall-clock, CPU time\n",
    "- **Throughput**: Rays/second, pixels/second\n",
    "- **Memory**: Peak usage, allocations\n",
    "- **Cache**: Hit rate, misses\n",
    "\n",
    "### Amdahl's Law\n",
    "\n",
    "Maximum speedup from parallelization:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{(1-P) + \\frac{P}{N}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P$ = fraction of code that can be parallelized\n",
    "- $N$ = number of processors\n",
    "\n",
    "Example: If 90% parallelizable with 8 cores:\n",
    "$$\n",
    "S = \\frac{1}{0.1 + \\frac{0.9}{8}} \\approx 4.7\\times\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Simple profiling timer\"\"\"\n",
    "    def __init__(self, name=\"Operation\"):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"{self.name}: {elapsed:.4f} seconds\")\n",
    "\n",
    "def benchmark(func, *args, iterations=10):\n",
    "    \"\"\"Benchmark function with multiple iterations\"\"\"\n",
    "    times = []\n",
    "    for _ in range(iterations):\n",
    "        start = time.time()\n",
    "        func(*args)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return mean_time, std_time\n",
    "\n",
    "print(\"✓ Profiling tools loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithmic Optimizations\n",
    "\n",
    "### Early Ray Termination\n",
    "\n",
    "Stop tracing when contribution becomes negligible:\n",
    "\n",
    "$$\n",
    "\\text{throughput} < \\epsilon \\implies \\text{terminate}\n",
    "$$\n",
    "\n",
    "### Bounding Volume Hierarchy (Review)\n",
    "\n",
    "- Use SAH for optimal splits\n",
    "- Traverse in front-to-back order\n",
    "- Cache-friendly memory layout\n",
    "\n",
    "### Incremental Computation\n",
    "\n",
    "Reuse calculations across iterations:\n",
    "- Don't recompute static geometry\n",
    "- Cache intersection data\n",
    "- Incremental updates for animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Naive vs optimized matrix-vector multiplication\n",
    "\n",
    "def matmul_naive(matrix, vector):\n",
    "    \"\"\"Naive matrix-vector multiply (list of lists)\"\"\"\n",
    "    n = len(matrix)\n",
    "    result = [0] * n\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            result[i] += matrix[i][j] * vector[j]\n",
    "    return result\n",
    "\n",
    "def matmul_numpy(matrix, vector):\n",
    "    \"\"\"NumPy optimized (BLAS)\"\"\"\n",
    "    return np.dot(matrix, vector)\n",
    "\n",
    "# Benchmark\n",
    "size = 100\n",
    "mat_list = [[float(i + j) for j in range(size)] for i in range(size)]\n",
    "vec_list = [float(i) for i in range(size)]\n",
    "mat_np = np.array(mat_list)\n",
    "vec_np = np.array(vec_list)\n",
    "\n",
    "with Timer(\"Naive matmul\"):\n",
    "    for _ in range(100):\n",
    "        matmul_naive(mat_list, vec_list)\n",
    "\n",
    "with Timer(\"NumPy matmul\"):\n",
    "    for _ in range(100):\n",
    "        matmul_numpy(mat_np, vec_np)\n",
    "\n",
    "print(\"NumPy uses optimized BLAS routines (often 10-100x faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structure Optimization\n",
    "\n",
    "### Array of Structures (AoS) vs Structure of Arrays (SoA)\n",
    "\n",
    "**AoS** (poor cache locality):\n",
    "```python\n",
    "vertices = [Vertex(x, y, z), Vertex(x, y, z), ...]\n",
    "```\n",
    "\n",
    "**SoA** (better cache locality):\n",
    "```python\n",
    "vertices_x = [x1, x2, x3, ...]\n",
    "vertices_y = [y1, y2, y3, ...]\n",
    "vertices_z = [z1, z2, z3, ...]\n",
    "```\n",
    "\n",
    "### Memory Layout\n",
    "\n",
    "- **Contiguous memory**: Better cache performance\n",
    "- **Alignment**: Align to cache line boundaries (64 bytes)\n",
    "- **Padding**: Avoid false sharing in multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate AoS vs SoA performance\n",
    "\n",
    "class Vec3AoS:\n",
    "    \"\"\"Array of Structures\"\"\"\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x, self.y, self.z = x, y, z\n",
    "\n",
    "def process_aos(vertices):\n",
    "    \"\"\"Process AoS\"\"\"\n",
    "    result = 0.0\n",
    "    for v in vertices:\n",
    "        result += v.x + v.y + v.z\n",
    "    return result\n",
    "\n",
    "def process_soa(x, y, z):\n",
    "    \"\"\"Process SoA\"\"\"\n",
    "    return np.sum(x) + np.sum(y) + np.sum(z)\n",
    "\n",
    "# Create data\n",
    "n = 100000\n",
    "vertices_aos = [Vec3AoS(i, i+1, i+2) for i in range(n)]\n",
    "vertices_x = np.arange(n, dtype=np.float64)\n",
    "vertices_y = np.arange(n, dtype=np.float64) + 1\n",
    "vertices_z = np.arange(n, dtype=np.float64) + 2\n",
    "\n",
    "# Benchmark\n",
    "with Timer(\"AoS processing\"):\n",
    "    for _ in range(10):\n",
    "        process_aos(vertices_aos)\n",
    "\n",
    "with Timer(\"SoA processing\"):\n",
    "    for _ in range(10):\n",
    "        process_soa(vertices_x, vertices_y, vertices_z)\n",
    "\n",
    "print(\"SoA is faster due to better cache locality and vectorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Threading\n",
    "\n",
    "### Thread-Based Parallelism\n",
    "\n",
    "Python's `threading` for I/O-bound tasks (GIL limitation)\n",
    "Python's `multiprocessing` for CPU-bound tasks\n",
    "\n",
    "### Rendering Parallelization Strategies\n",
    "\n",
    "1. **Tile-based**: Divide image into tiles, one per thread\n",
    "2. **Scanline**: Each thread renders different rows\n",
    "3. **Interleaved**: Thread $i$ renders every $n$-th pixel\n",
    "4. **Dynamic**: Work queue with tasks\n",
    "\n",
    "### Thread Synchronization\n",
    "\n",
    "- **Locks**: Protect shared data\n",
    "- **Atomics**: Lock-free operations\n",
    "- **Thread-local storage**: Avoid sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_tile(x_start, y_start, tile_size, width, height):\n",
    "    \"\"\"Render a tile of pixels (dummy implementation)\"\"\"\n",
    "    tile = np.zeros((tile_size, tile_size, 3))\n",
    "    \n",
    "    for y in range(tile_size):\n",
    "        for x in range(tile_size):\n",
    "            px = x_start + x\n",
    "            py = y_start + y\n",
    "            \n",
    "            if px < width and py < height:\n",
    "                # Simulate ray tracing work\n",
    "                tile[y, x] = [px / width, py / height, 0.5]\n",
    "    \n",
    "    return (x_start, y_start, tile)\n",
    "\n",
    "def render_single_threaded(width, height, tile_size):\n",
    "    \"\"\"Single-threaded tiled rendering\"\"\"\n",
    "    image = np.zeros((height, width, 3))\n",
    "    \n",
    "    for y in range(0, height, tile_size):\n",
    "        for x in range(0, width, tile_size):\n",
    "            x_start, y_start, tile = render_tile(x, y, tile_size, width, height)\n",
    "            \n",
    "            # Copy tile to image\n",
    "            h = min(tile_size, height - y_start)\n",
    "            w = min(tile_size, width - x_start)\n",
    "            image[y_start:y_start+h, x_start:x_start+w] = tile[:h, :w]\n",
    "    \n",
    "    return image\n",
    "\n",
    "def render_multi_threaded(width, height, tile_size, num_threads=None):\n",
    "    \"\"\"Multi-threaded tiled rendering\"\"\"\n",
    "    if num_threads is None:\n",
    "        num_threads = cpu_count()\n",
    "    \n",
    "    image = np.zeros((height, width, 3))\n",
    "    \n",
    "    # Generate tile jobs\n",
    "    jobs = []\n",
    "    for y in range(0, height, tile_size):\n",
    "        for x in range(0, width, tile_size):\n",
    "            jobs.append((x, y, tile_size, width, height))\n",
    "    \n",
    "    # Process tiles in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = executor.map(lambda args: render_tile(*args), jobs)\n",
    "        \n",
    "        for x_start, y_start, tile in results:\n",
    "            h = min(tile_size, height - y_start)\n",
    "            w = min(tile_size, width - x_start)\n",
    "            image[y_start:y_start+h, x_start:x_start+w] = tile[:h, :w]\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"✓ Multi-threading loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SIMD Vectorization\n",
    "\n",
    "**SIMD** (Single Instruction, Multiple Data) processes multiple values simultaneously.\n",
    "\n",
    "### Vector Operations\n",
    "\n",
    "Modern CPUs support:\n",
    "- **SSE**: 128-bit (4 floats)\n",
    "- **AVX**: 256-bit (8 floats)\n",
    "- **AVX-512**: 512-bit (16 floats)\n",
    "\n",
    "### Ray Packet Tracing\n",
    "\n",
    "Trace 4/8/16 coherent rays together:\n",
    "- Better cache utilization\n",
    "- Amortize BVH traversal\n",
    "- SIMD intersection tests\n",
    "\n",
    "### NumPy Vectorization\n",
    "\n",
    "NumPy automatically uses SIMD for array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization example: computing distances\n",
    "\n",
    "def distance_scalar(points, center):\n",
    "    \"\"\"Scalar computation (slow)\"\"\"\n",
    "    distances = []\n",
    "    for p in points:\n",
    "        dx = p[0] - center[0]\n",
    "        dy = p[1] - center[1]\n",
    "        dz = p[2] - center[2]\n",
    "        dist = math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "        distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "def distance_vectorized(points, center):\n",
    "    \"\"\"Vectorized computation (fast)\"\"\"\n",
    "    diff = points - center\n",
    "    return np.sqrt(np.sum(diff * diff, axis=1))\n",
    "\n",
    "# Generate test data\n",
    "n_points = 10000\n",
    "points_list = [[np.random.rand(), np.random.rand(), np.random.rand()] \n",
    "               for _ in range(n_points)]\n",
    "points_np = np.random.rand(n_points, 3)\n",
    "center = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "# Benchmark\n",
    "with Timer(\"Scalar distance\"):\n",
    "    distance_scalar(points_list, center)\n",
    "\n",
    "with Timer(\"Vectorized distance\"):\n",
    "    distance_vectorized(points_np, center)\n",
    "\n",
    "print(\"Vectorized version is typically 10-100x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPU Acceleration Concepts\n",
    "\n",
    "### CUDA/OpenCL Architecture\n",
    "\n",
    "- **Massive parallelism**: Thousands of threads\n",
    "- **Memory hierarchy**: Global, shared, local, constant\n",
    "- **Warps/Wavefronts**: Groups of threads execute together\n",
    "- **Divergence**: Branching reduces efficiency\n",
    "\n",
    "### Ray Tracing on GPU\n",
    "\n",
    "**One thread per pixel**:\n",
    "```cuda\n",
    "int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "if (x < width && y < height) {\n",
    "    Ray ray = generate_camera_ray(x, y);\n",
    "    Color c = trace_ray(ray, scene);\n",
    "    image[y * width + x] = c;\n",
    "}\n",
    "```\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "1. **Coalesced memory access**: Access consecutive memory\n",
    "2. **Shared memory**: Cache frequently accessed data\n",
    "3. **Occupancy**: Balance registers/shared memory/threads\n",
    "4. **Avoid divergence**: Minimize if/else branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual GPU pseudo-code (not executable Python)\n",
    "gpu_code = \"\"\"\n",
    "// CUDA kernel for ray tracing\n",
    "__global__ void raytrace_kernel(\n",
    "    float3* image,\n",
    "    const Camera camera,\n",
    "    const Scene* scene,\n",
    "    int width, int height)\n",
    "{\n",
    "    // Thread coordinates\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    \n",
    "    if (x >= width || y >= height) return;\n",
    "    \n",
    "    // Generate camera ray\n",
    "    float u = (float)x / width;\n",
    "    float v = (float)y / height;\n",
    "    Ray ray = camera.generate_ray(u, v);\n",
    "    \n",
    "    // Trace ray\n",
    "    float3 color = trace_ray(ray, scene, 0);\n",
    "    \n",
    "    // Write to image\n",
    "    image[y * width + x] = color;\n",
    "}\n",
    "\n",
    "// Launch kernel\n",
    "dim3 blockSize(16, 16);\n",
    "dim3 gridSize(\n",
    "    (width + blockSize.x - 1) / blockSize.x,\n",
    "    (height + blockSize.y - 1) / blockSize.y\n",
    ");\n",
    "raytrace_kernel<<<gridSize, blockSize>>>(image, camera, scene, width, height);\n",
    "\"\"\"\n",
    "\n",
    "print(\"GPU code example (CUDA):\")\n",
    "print(gpu_code)\n",
    "print(\"\\nKey points:\")\n",
    "print(\"- One thread per pixel\")\n",
    "print(\"- Launched in 16x16 thread blocks\")\n",
    "print(\"- Massively parallel (thousands of threads)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multi-Threading Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs multi-threaded rendering\n",
    "width, height = 512, 512\n",
    "tile_size = 64\n",
    "\n",
    "print(f\"Rendering {width}x{height} image with {tile_size}x{tile_size} tiles\")\n",
    "print(f\"Available CPU cores: {cpu_count()}\\n\")\n",
    "\n",
    "# Single-threaded\n",
    "with Timer(\"Single-threaded\"):\n",
    "    img_single = render_single_threaded(width, height, tile_size)\n",
    "\n",
    "# Multi-threaded\n",
    "for num_threads in [2, 4, cpu_count()]:\n",
    "    with Timer(f\"Multi-threaded ({num_threads} threads)\"):\n",
    "        img_multi = render_multi_threaded(width, height, tile_size, num_threads)\n",
    "\n",
    "# Visualize result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(img_single)\n",
    "axes[0].set_title('Single-threaded Result')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_multi)\n",
    "axes[1].set_title('Multi-threaded Result')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-threading provides near-linear speedup for embarrassingly parallel tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Optimization and parallelization** strategies for production rendering:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Profiling First**\n",
    "   - Measure before optimizing\n",
    "   - Identify hotspots\n",
    "   - Amdahl's law limits\n",
    "\n",
    "2. **Algorithmic Optimization**\n",
    "   - BVH acceleration\n",
    "   - Early termination\n",
    "   - Incremental computation\n",
    "   - Cache-friendly data structures\n",
    "\n",
    "3. **Data Structure Layout**\n",
    "   - SoA vs AoS: SoA usually better\n",
    "   - Memory alignment\n",
    "   - Contiguous storage\n",
    "   - Avoid false sharing\n",
    "\n",
    "4. **Multi-Threading**\n",
    "   - Tile-based parallelism\n",
    "   - Thread pool\n",
    "   - Lock-free when possible\n",
    "   - Near-linear speedup for rendering\n",
    "\n",
    "5. **SIMD Vectorization**\n",
    "   - Process 4-16 values at once\n",
    "   - Ray packet tracing\n",
    "   - NumPy auto-vectorizes\n",
    "   - 10-100x faster than scalar\n",
    "\n",
    "6. **GPU Acceleration**\n",
    "   - Massive parallelism (1000s of threads)\n",
    "   - One thread per pixel/ray\n",
    "   - Coalesced memory access\n",
    "   - Avoid divergence\n",
    "   - 10-100x faster than CPU\n",
    "\n",
    "### Performance Hierarchy\n",
    "\n",
    "```\n",
    "Naive Python:           1x      (baseline)\n",
    "NumPy vectorized:       10-100x (SIMD)\n",
    "Multi-threaded CPU:     4-16x   (cores)\n",
    "Combined (NumPy+MT):    40-1600x\n",
    "GPU (CUDA):             100-1000x\n",
    "```\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "✅ Profile to find bottlenecks  \n",
    "✅ Use efficient algorithms (BVH, etc.)  \n",
    "✅ Vectorize with NumPy  \n",
    "✅ Use SoA data layout  \n",
    "✅ Multi-thread embarrassingly parallel tasks  \n",
    "✅ Consider GPU for massive parallelism  \n",
    "\n",
    "### Production Renderer Performance\n",
    "\n",
    "Modern renderers combine all techniques:\n",
    "- **Pixar RenderMan**: Multi-core CPU + GPU hybrid\n",
    "- **Arnold**: Multi-threaded with SIMD\n",
    "- **V-Ray**: CPU + GPU modes\n",
    "- **Cycles**: OpenCL/CUDA GPU acceleration\n",
    "- **Real-time (games)**: GPU ray tracing (RTX)\n",
    "\n",
    "Optimization is essential for interactive and production rendering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
